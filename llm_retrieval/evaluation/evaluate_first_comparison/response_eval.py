import json
from datasets import load_dataset
import os
from dotenv import load_dotenv
import requests

load_dotenv()
EDENAI_API_KEY = os.getenv("EDENAI_API_KEY")
dataset = load_dataset("json", data_files="/mnt/c/Users/User/thesis/data_import/exp2/rag_output_qa_dataset_exp2.json", split="train")
url = "https://api.edenai.run/v2/text/chat"


headers = {
    "Authorization": f"Bearer {EDENAI_API_KEY}",
    "accept": "application/json",
    "content-type": "application/json"
}


global_message = """
You are an evaluator tasked with determining whether a question-answer pair should be approved or disapproved 
based on the following criteria.
The original data is part of a parliament debate from the Swedish parliament (riksdagen), and the questions and
answers are generated by an LLM from random statements by politicians. The questions have been validated by a human,
but you must evaluate the answers.


You should only answer in the format below.
### Output Format:
{"Quality":True/False, "Relevance":True/False}

The context, question, and answer will be provided.
Do **NOT** add any explanation.
Do not hallucinate. My master thesis depends on it, so do a good job, please!
"""


def evaluate_rag_output_with_edenai(question, answer, context):
    prompt = f"""   
            You will base your evaluation on two of Grice's maxims: quality and relevance.
            You should base your evaluation on the follwoing context: 
            {context}
            This is the question being asked by a user:
            {question}
            And this is the answer you are meant to evaluate:
            {answer}

            Quality  
            True if: 
            The answer is factually accurate.
            The answer is based on the provided context.

            False if:  
            The answer contains fabricated, misleading, or unverifiable claims.
            The answer is not based on the provided context.

            Relevance  
            True if:  
            The answer provides relevant information based on the provided question. Meaning does the provided answer actually answer the question.
            
            False if:  
            The response includes unrelated content or goes off-topic.
  
            
            Always False if:
            If the response is empty or does not anser the questions, you should evaluate it as false.
            If the response says that the topic is not discussed, make sure to validate this in the provided context.
            If the information to evaluate the question is in the provided context, it should be evaluated as false.
            """
    payload = {
        "providers": "openai/gpt-4o",#"openai/gpt-4o", #"openai/gpt-4o",  # "openai/gpt-4o",   
        "response_as_dict": True,
        "attributes_as_list": False,
        "show_base_64": True,
        "show_original_response": False,
        "temperature": 0,
        "max_tokens": 4096,
        "tool_choice": "auto",
        "previous_history": [
            {'role': 'user', 'message': prompt}, 
            {'role': 'user', 'message': global_message}
        ]
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code == 200:
        evaluation = response.json()
        return evaluation
    else:
        print("Error with EdenAI API request:", response.text)
        return None


updated_data = list()

for idx, entry in enumerate(dataset):
    if 'eval' not in entry:
        entry['eval'] = {}

    question = entry.get("qa_pair", {}).get("question", "N/A")
    context = entry.get("anforandetext", "N/A")
    original_answer = entry.get("qa_pair", {}).get("answer", "N/A")
    print("original: ", question, "context: ", context, "answer: ", original_answer)
    graph_rag_answer = entry.get("graph_RAG", {}).get("answer", "N/A")
    print("rag_answer: ", graph_rag_answer)
    cosine_rag_answer = entry.get("cosine_RAG", {}).get("answer", "N/A")
    
    evaluation_original = evaluate_rag_output_with_edenai(question, original_answer, context)
    evaluation_graph_rag = evaluate_rag_output_with_edenai(question, graph_rag_answer, context)
    evaluation_cosine_rag = evaluate_rag_output_with_edenai(question, cosine_rag_answer, context)
    print(evaluation_original)
    if evaluation_original:
        if "openai/gpt-4o" in evaluation_original and "generated_text" in evaluation_original["openai/gpt-4o"]:
            response = evaluation_original["openai/gpt-4o"]['generated_text']
            entry["eval"]["orig_answer"] = response
            print("evaluation_reasonable_answer_original", response)
        else:
            print(f"Skipping entry {idx} due to content policy violation.")
            entry["eval"]["orig_answer"] = "Content rejected due to policy violation."
    
    print("="*80)
    
    if evaluation_graph_rag:
        if "generated_text" in evaluation_graph_rag["openai/gpt-4o"]:
            response = evaluation_graph_rag["openai/gpt-4o"]['generated_text']
            entry["eval"]["graph_RAG_response"] = response
            print("graph_RAG_response", response)
        else:
            print(f"Skipping entry {idx} due to content policy violation.")
            entry["eval"]["graph_RAG_response"] = "Content rejected due to policy violation."
    
    if evaluation_cosine_rag:
        if "openai/gpt-4o" in evaluation_cosine_rag and "generated_text" in evaluation_cosine_rag["openai/gpt-4o"]:
            response = evaluation_cosine_rag["openai/gpt-4o"]['generated_text']
            entry["eval"]["cosine_RAG_response"] = response
            print("cosine_RAG_response", response)
        else:
            print(f"Skipping entry {idx} due to content policy violation.")
            entry["eval"]["cosine_RAG_response"] = "Content rejected due to policy violation."
    
    updated_data.append(entry)


with open("/mnt/c/Users/User/thesis/llm_retrieval/evaluation/evaluate_first_comparison/evaluated_dataset_exp1.json", "w", encoding="utf-8") as f:
    json.dump(updated_data, f, ensure_ascii=False, indent=4)

print("Dataset successfully updated and saved!")
