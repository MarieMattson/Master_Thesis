import requests
import json
import os
from dotenv import load_dotenv
from datasets import load_dataset

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
N_GENERATIONS = 5  # Generate only 5 QA pairs for cost and time considerations
dataset = load_dataset("json", data_files="/mnt/c/Users/User/thesis/data_import/exp1/even_more_filtered_riksdag.json", split="train")
#dataset = dataset.filter(lambda x: x["dok_id"] in ["H90968", "H90982"])
url = "https://api.openai.com/v1/chat/completions"


system_prompt = """
                Your task is to generate factoid question-answer pairs in **Swedish**, based on **spoken statements ("anforandetext") from debates in the Swedish parliament (Riksdagen).** 

                ### **Important Instructions**
                - Each **factoid question** must:
                - Be written in **natural Swedish**, as if a user were searching online.
                - Be **factually correct** and based on actual statements made in the debate.
                - **Not mention metadata fields** like "systemdatum", "anforande_id", or dataset structure.
                - **Focus on arguments and discussions** from members of parliament.
                - Be **specific** and answerable with a **concise, factual response**.

                - Each **answer** must:
                - Provide a **concise and factual** response based on the debate text.
                - Be **directly supported** by a quoted excerpt from "anforandetext."

                ---

                ### **Output Format**
                Each factoid QA pair must follow this exact structure:

                Factoid question: (Your generated factoid question)
                Answer: (Your factually correct answer based on the context)
                Context: (The "anforande_id" that supports your answer)

                Examples:
                Factoid question: Hur argumenterar Anders Borg om restaurangmomsen?
                Answer: Anders Borg menar att s√§nkt restaurangmoms leder till fler jobb inom restaurangbranschen.
                Context: "59a228e8-3dc6-ec11-9170-0090facf175a"

                Factoid question: Hur debatterar V√§nsterpartiet om vinster i v√§lf√§rden?
                Answer: V√§nsterpartiet argumenterar f√∂r att vinster i v√§lf√§rden b√∂r begr√§nsas f√∂r att s√§kerst√§lla att skattepengar g√•r till omsorg och utbildning, inte till privata akt√∂rer.
                Context: "5aa228e8-3dc6-ec11-9170-0090facf175a"

                ### **üö´ Avoid These Mistakes**
                ‚ùå **DO NOT generate questions about dataset structure** (e.g., "Vad betyder anforande_nummer?")  
                ‚ùå **DO NOT write any additional text except what is instructed above** (e.g., "H√§r kommer fr√•gor om...")  
                ‚ùå **DO NOT create vague or opinion-based questions** (e.g., "Vad tycker folk om skatter?")  
                """
def generate_qa_from_openai(context, anforande_id, bonus_prompt):
    body = {
        "model": "gpt-4o",
        "messages": [
            {"role": "system", "content": bonus_prompt+system_prompt},
            {"role": "user", "content": f"Context:\n{context}"}
        ],
        "max_tokens": 500,
        "temperature": 0
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {OPENAI_API_KEY}"
    }

    response = requests.post(url, json=body, headers=headers)

    if response.status_code == 200:
        data = response.json()
        response_text = data["choices"][0]["message"]["content"]

        # Split the response into lines
        qa_lines = response_text.strip().split("\n")

        # Check if there are at least two lines for a question and an answer
        if len(qa_lines) >= 2:
            question_line = qa_lines[0].replace("Factoid question:", "").strip()
            answer_line = qa_lines[1].replace("Answer:", "").strip()

            # Return a single question-answer pair
            return {"question": question_line, "answer": answer_line, "source_doc": anforande_id}
        else:
            return None
    else:
        print("Error:", response.text)
        return None

qa_results = list()
for example in dataset:
    anforandetext = example["anforandetext"]
    anforande_id = example["anforande_id"]
    theme = example["avsnittsrubrik"]
    speaker = example["talare"]
    party = example["parti"]

    prompt_addon = f"""
                    Generate a question-answer pair based on this statement by {speaker} from the party {party}
                    From the debate about {theme}.

                    Start the question like this:
                    "Vad s√§ger {speaker} om..." or "Hur argumenterar {speaker} f√∂r..."

                    And always start the response by mentioning {speaker}'s name.

                    ALWAYS write both a question and an answer!!
                    """
    
    qa_pair = generate_qa_from_openai(anforandetext, anforande_id, prompt_addon)

    if qa_pair:
        qa_results.append({
            "anforande_id": example["anforande_id"],
            "dok_id": example["dok_id"],
            "talare": example["talare"],
            "question": qa_pair["question"],
            "answer": qa_pair["answer"],
            "context": anforandetext
        })

print(json.dumps(qa_results, ensure_ascii=False, indent=2))
'''


Use for future prompting for getting other kinds of questions
                Comparison questions or yes/no questions
                For example: "Tycker Carl Bildt att arbetsgivaravgiten ska h√∂jas?"
                Temporal or time related questions:
                For example: "Vad argumenterade Jonas Sj√∂stedt f√∂r i maj 2018?"
                And null questions, meaning questions that cannot be answered given the database:
                For example: "Vad tycker Johan Pehrsson om vargjakt?"

previous_history = [
    {"role": "user", "message": """Generate a question-answer pair in Swedish based on what Anders Borg says in the context below.

    Context:
    Anders Borg: \r\nFru talman! Utg√•ngspunkten f√∂r den h√§r debatten √§r fr√•gan om regeringens ekonomiska politik har fungerat.\r\nV√§rlden befinner sig i den kanske djupaste och l√§ngsta recession som vi har upplevt sedan depressionen. Land efter land har fallit ned i en ekonomisk abyss.\r\nSverige st√•r ut j√§mf√∂rt med n√§stan alla industril√§nder n√§r man tittar samlat p√• utvecklingen. Det √§r en b√§ttre tillv√§xt, h√∂gre reall√∂ner, starkare konsumtionsutveckling, h√∂g syssels√§ttningsgrad och l√•g l√•ngtidsarbetsl√∂shet. Det √§r inkomstskillnader som inte har √∂kat som de har gjort i andra l√§nder och en minskad andel som lever med l√•g materiell standard, tack vare att vi har kunnat h√∂ja konsumtionen i Sverige, trots att det har varit en kraftig l√•gkonjunktur.\r\nDet h√§r beror naturligtvis p√• att vi har f√∂rt en samlad ekonomisk politik som pr√§glas av ansvar, arbete och kunskap. Jobbskatteavdrag och f√∂r√§ndringar i socialf√∂rs√§kringen och f√∂r√§ndringar av a-kassa har gjort att det l√∂nar sig b√§ttre att arbeta.\r\nNystartsjobb och f√∂r√§ndringar av arbetsmarknadspolitiken har gjort att vi st√∂ttar dem som har sv√•righeter p√• arbetsmarknaden. Oml√§ggning av utbildningspolitiken i riktning mot mer av kunskap, betyg och prov g√∂r att vi l√§gger en starkare grund. B√§ttre f√∂retagsklimat med borttagen arvsskatt och f√∂rm√∂genhetsskatt och s√§nkt bolagsskatt g√∂r att Sverige st√•r sig v√§l.\r\nNu h√§vdar debatt√∂rerna h√§r att vi inte har haft n√•gra forskare som sagt att regeringens politik kan ha bidragit till detta. Jag vill bara p√•peka att KI:s bed√∂mning √§r att n√•gonstans runt 1 procent i l√§gre l√•ngsiktig arbetsl√∂shet √§r effekten av regeringens politik.\r\nRiksbanken s√§ger att det √§r ¬Ω till 1 ¬Ω procent eller 1 ¬Ω till 2 procent, om jag minns siffrorna r√§tt. Finansdepartementet landar p√• siffror som ligger v√§ldigt n√§ra Riksbankens och KI:s. Vi har OECD och IMF och Finanspolitiska r√•det som s√§ger exakt samma sak.\r\nSocialdemokraterna f√∂rs√∂ker driva en tes, det vill s√§ga att marginaleffekten, v√§rdet av att arbeta n√§r man tar h√§nsyn till skatter och bidrag, inte ska p√•verka arbetsutbudet. Det √§r Socialdemokraternas tes.\r\nDet finns helt enkelt inga forskare som st√∂der den slutsatsen, utan forskningen √§r klar och tydlig. Hur mycket det l√∂nar sig att arbeta och hur v√§l l√∂nebildningen fungerar √§r det som avg√∂r hur stort arbetsutbud vi f√•r. Och det √§r det som best√§mmer den l√•ngsiktiga syssels√§ttningsgraden.\r\nJag m√•ste s√§ga att jag √§r tacksam f√∂r att Socialdemokraterna g√∂r att vi kommer att f√• en tydlig och bra valr√∂relse.\r\nSocialdemokraterna f√∂rordar en politik d√§r det ena benet √§r stora skatteh√∂jningar. H√∂jda arbetsgivaravgifter f√∂r unga, h√∂jd restaurangmoms och h√∂jd bolagsskatt √§r h√∂rnpelarna. Det ska anv√§ndas till en bred utbyggnad av bidragssystemen. Praktiskt taget varje bidragssystem ska byggas ut. Det som tillkommit under den h√§r senaste budgetomg√•ngen √§r barnbidraget. Men det √§r ocks√• a-kassan. Det √§r sjukf√∂rs√§kringen. F√∂rtidspensioneringen ska √∂ppnas och s√• vidare.\r\nI en v√§lf√§rdsstat med en √•ldrande befolkning har allts√• Socialdemokraterna det relativt unika receptet att man ska h√∂ja skatterna p√• arbete och efterfr√•gan f√∂r att kraftigt bygga ut socialf√∂rs√§kringssystemet s√• att fler m√§nniskor l√§mnar arbetsmarknaden. Det √§r en v√§ldigt ovanlig ekonomisk politik.\r\nVad v√§rre √§r √§r att Socialdemokraterna t√§nker genomf√∂ra den h√§r politiken med Milj√∂partiet, och Milj√∂partiet har en egen id√© om ekonomisk politik. Det √§r en stor satsning p√• milj√∂investeringar som ska finansieras med kraftigt h√∂jda bensin- och transportskatter. Det √§r Milj√∂partiets politik.\r\nL√§gger man ihop den h√§r bilden blir det √§nnu m√§rkligare. F√∂rst ska vi sk√§ra ned efterfr√•gan med h√∂jd moms, h√∂jd bensinskatt och h√∂jda transportskatter. Sedan ska vi l√§gga p√• kostnader i form av h√∂jda arbetsgivaravgifter, h√∂jd bolagsskatt och f√∂rs√§mringar av f√∂retagsklimatet. Det √§r klart att den √•tstramningspolitiken kommer att leda till att f√§rre m√§nniskor har arbete.\r\nN√§r man sedan l√§gger p√• att denna politik anv√§nds i syfte att bygga ut bidragssystemen s√• att fler m√§nniskor ska l√§mna arbetsmarknaden kan man ju inte f√• n√•gon annan effekt √§n att om man g√∂r det dyrare och mindre l√∂nsamt att arbeta, d√• l√§mnar m√§nniskor arbetsmarknaden, och d√• stiger arbetsl√∂sheten och faller syssels√§ttningen.\r\n"""
    }
]

payload = {
    "providers": "deepseek/DeepSeek-V3", #"meta/llama3-1-405b-instruct-v1:0",
    "response_as_dict": True,
    "attributes_as_list": False,
    "show_base_64": True,
    "show_original_response": False,
    "temperature": 0,
    "max_tokens": 4096,
    "tool_choice": "auto",
    "previous_history": previous_history,
    "chatbot_global_action": global_message
}

headers = {
    "Authorization": f"Bearer {EDENAI_API_KEY}",
    "accept": "application/json",
    "content-type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)



print(f"Generating {N_GENERATIONS} QA couples...")

outputs = []
for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):
    # Generate QA couple using Hugging Face Llama model
    prompt = QA_generation_prompt.format(context=sampled_context.page_content)
    
    try:
        # Call the model with the prompt
        response = llm(prompt)[0]["generated_text"]  # Access the first generated text
        
        # Now split the response properly
        question = response.split("Faktabaserad fr√•ga: ")[-1].split("Svar: ")[0].strip()
        answer = response.split("Svar: ")[-1].strip()

        # Ensure the answer is not too long
        assert len(answer) < 300, "Answer is too long"

        # Append the result
        outputs.append(
            {
                "context": sampled_context.page_content,
                "question": question,
                "answer": answer,
                "source_doc": sampled_context.metadata["source"],
            }
        )
    except Exception as e:
        print(f"Error generating QA pair for document {sampled_context.metadata['source']}: {e}")
        continue

print(outputs)


model_id = "meta-llama/Meta-Llama-3-8B"

pipeline = transformers.pipeline(
    "text-generation", model=model_id, model_kwargs={"torch_dtype": torch.bfloat16}, device_map="auto"
)

dataset = load_dataset("json", data_files="/mnt/c/Users/User/thesis/data_import/filtered_riksdag.json")
dataset = dataset.filter(lambda x: x["dok_id"] == "H00998")

# Display options for pandas
#pd.set_option("display.max_colwidth", None)

# Login to Hugging Face if needed
notebook_login()

# Prepare documents
langchain_docs = [
    LangchainDocument(page_content=doc["anforandetext"], metadata={"source": doc["dok_id"]}) 
    for doc in tqdm(dataset["train"])
]

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=200,
    add_start_index=True,
    separators=["\n\n", "\n", ".", " ", ""],
)

docs_processed = []
for doc in langchain_docs:
    docs_processed += text_splitter.split_documents([doc])

QA_generation_prompt = """
                        Du m√•ste generera fr√•gor och svar p√• svenska.
                        Fr√•gorna ska r√∂ra inneh√•llet i debatten. 
                        Exempelvis fr√•gor om vad olika politiker tycker om sakfr√•gor som diskuteras.
                        Ett exempel kan vara "Vad p√•st√•r Anders Borg om restuarangmoms?".
                        Your task is to write a factoid question and an answer given a context.
                        Your factoid question should be answerable with a specific, concise piece of factual information from the context.
                        Your factoid question should be formulated in the same style as questions users could ask in a search engine.
                        This means that your factoid question MUST NOT mention something like "according to the passage" or "context".

                        Provide your answer as follows:

                        Output:::
                        Factoid question: (your factoid question)
                        Answer: (your answer to the factoid question)

                        Now here is the context.

                        Context: {context}\n
                        Output:::"""


N_GENERATIONS = 10  # Generate only 10 QA couples here for cost and time considerations

print(f"Generating {N_GENERATIONS} QA couples...")

outputs = []
for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):
    # Generate QA couple using Hugging Face Llama model
    prompt = QA_generation_prompt.format(context=sampled_context.page_content)
    
    try:
        # Call the model with the prompt
        response = llm(prompt)[0]["generated_text"]  # Access the first generated text
        
        # Now split the response properly
        question = response.split("Faktabaserad fr√•ga: ")[-1].split("Svar: ")[0].strip()
        answer = response.split("Svar: ")[-1].strip()

        # Ensure the answer is not too long
        assert len(answer) < 300, "Answer is too long"

        # Append the result
        outputs.append(
            {
                "context": sampled_context.page_content,
                "question": question,
                "answer": answer,
                "source_doc": sampled_context.metadata["source"],
            }
        )
    except Exception as e:
        print(f"Error generating QA pair for document {sampled_context.metadata['source']}: {e}")
        continue

print(outputs)'''
